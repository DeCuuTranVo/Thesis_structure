1. Read paper
    Convolutional Autoencoders
2. Refactor code structures
    almost done 
        class imbalance?
            -> oversampling
                https://github.com/ufoym/imbalanced-dataset-sampler
                train or val - random sampler?

                ImbalancedDatasetSampler(train_dataset)
                weighted sample testset ?
            -> weighted loss: ok

        restructure config file

        F.one_hot ?
        F.ravel ?

        test.py -> ok        
        predict.py  -> ok
        
        output_file tmux
        utils.py 
            classification report - test.py -> ok
            confusion matrix - test.py -> ok
            gradcam - predict.py
        

        augmentation -> train model
                            spatial
                            intensity 


        test on 2 classes: AD vs NC, NC vs EMCI
        metrics -> need add sensitivity/ specitivity
        add efficientnet -> done
        TRIAL
        DEBUG_DATASET
        regularization ->>>
        softmax or logsoftmax or logit in model.py
        early stopping, monitoring ->>

        

5. Predict 2 class

3. Train on full dataset
    -> train in parallel


6. GradCAM          => tommorrow
    model -> get layer 
    predict -> get image
    inspect attention map -> super impose


4. 5 fold cross validation

7. Augmentation
https://discuss.pytorch.org/t/data-augmentor-for-3d-images/30986/4
    -> Augmentation 3D image 
    TorchIO
    https://torchio.readthedocs.io/transforms/transforms.html
    https://pythonawesome.com/tools-for-augmenting-and-writing-3d-medical-images-on-pytorch/

    MONAI
    https://github.com/Project-MONAI/tutorials/blob/master/3d_classification/densenet_training_array.ipynb
    https://github.com/Project-MONAI/MONAI
    Volumentation


    torchIO: Augmentation
    MONAI:  Augmentation, Network
    pytorch Lighting: Code structures

    Check sklearn metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics


	"USE_TRAINED_MODEL" : true,
	"PATH_PRETRAINED_MODEL": "models/trial_3.pth",

Record
Trial 53:
    Training set: Average loss: 0.484853, Average accuracy: 73.830%, AUC score: 0.812

    Validation set: Average loss: 0.530972, Average accuracy: 75.214%, AUC score: 0.795

    Improve accuracy from 0.7436 to 0.7521
    Save model to models/trial_53.pth
    Epoch: 88  

Trial 44:
    Training set: Average loss: 0.695936, Average accuracy: 53.586%, AUC score: 0.545

    Validation set: Average loss: 0.734874, Average accuracy: 44.444%, AUC score: 0.700

    Improve accuracy from 0.4444 to 0.4444
    Save model to models/trial_44.pth
################################################



12/10/2021

1. Restructure class imbalance -> ok
2. Learning rate scheduler 
    how to see learning rate scheduler for each EPOCHS?
3. Upload code to github -> ok
4. Augmentation -> ok
    Spacial
        monai
            Random Crop?
            Random Rotate?
            RandFlip ?
            RandAffine ?
            RandZoom ?
    Random intensity, contrast
        monai
            HistogramNormalize
            RandScaleIntensity
            RandStdShiftIntensity
            RandAdjustConstrast

4. other 3D networks: 3D Densenet (monai), ResNext50 (monai), SENet154 (monai), MobileNet(Efficient-3DCNNs), ShuffleNet(Efficient-3DCNNs), SqueezeNet(Efficient-3DCNNs)
    Mobilenet v2, Shufflenet v2, Densenet121, SENet154 (monai), SE-ResNet50, 
    https://glassboxmedicine.com/2021/02/06/designing-custom-2d-and-3d-cnns-in-pytorch-tutorial-with-code/

    ShuffleNet: Input Dimension missmatch problem
    SqueezeNet: Output dimension mismatch problem

    Transformers

5, Hyper parameter tuner

7. Bigger image size

11. Cyclincal learning rate -> ok
    Mode: Triangular 2?
    Mode: Exp ?

12. Bayesian optimization on hyperparameter tuning
    Facebook ax_platform
        https://ax.dev/api/utils.html
    Hyperopt
10. 5-fold hold out cross validation

6. Parallel training -> ok

8. Convolutional Auto encoder
    Clone other repo: ALhosseni, Liu2019

9. accuracy for 3 classes

13. Brain 3D visualization
    https://www.kaggle.com/polomarco/visualizatio-3d-nifti-dicom-matlab-nrrd-files

Batchgenerators intensity augmentation 
https://github.com/MIC-DKFZ/batchgenerators

Note:
    Architecture
        https://theaisummer.com/cnn-architectures/
        https://www.kaggle.com/getting-started/149448
        https://paperswithcode.com/sota/image-classification-on-imagenet

Trial 71: ResNet50
Training set: Average loss: 1.019430, Average accuracy: 54.353%, AUC score: 0.803

Validation set: Average loss: 1.120408, Average accuracy: 56.219%, AUC score: 0.791

Improve accuracy from 0.5622 to 0.5622
Save model to models/trial_71.pth


Trial 72: SEResNet50
Training set: Average loss: 1.026767, Average accuracy: 55.473%, AUC score: 0.805

Validation set: Average loss: 1.088242, Average accuracy: 56.716%, AUC score: 0.797

Improve accuracy from 0.5672 to 0.5672
Save model to models/trial_72.pth


______________________________________________________________________________________________________

3 classes
Learning rate scheduler ->ok ?
    draw lr per epochs
Bayesian optimization -> read paper
5-fold cross-validation
read more paper
torch.optim.lr_scheduler.StepLR


batchsize 16:
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3173      G   /usr/lib/xorg/Xorg                118MiB |
|    0   N/A  N/A      3893      G   /usr/bin/gnome-shell               84MiB |
|    0   N/A  N/A     21810      C   python                           6551MiB |
|    1   N/A  N/A      3173      G   /usr/lib/xorg/Xorg                  0MiB |
|    1   N/A  N/A      3893      G   /usr/bin/gnome-shell                0MiB |
|    1   N/A  N/A     21810      C   python                           6485MiB |
+-----------------------------------------------------------------------------+

batchsize 8:
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3173      G   /usr/lib/xorg/Xorg                118MiB |
|    0   N/A  N/A      3893      G   /usr/bin/gnome-shell               85MiB |
|    0   N/A  N/A      7070      C   python                           3853MiB |
|    1   N/A  N/A      3173      G   /usr/lib/xorg/Xorg                  0MiB |
|    1   N/A  N/A      3893      G   /usr/bin/gnome-shell                0MiB |
|    1   N/A  N/A      7070      C   python                           3799MiB |
+-----------------------------------------------------------------------------+



Cyclic Exp-range: 
    "OPTIMIZER": "adam",
	"SCHEDULER": "cyclic",
	"LEARNING_RATE_START": 1e-5,

	"LEARNING_RATE_SCHEDULE_FACTOR": 0.9,
	"LEARNING_RATE_SCHEDULE_PATIENCE": 5,

	"MODE":"exp_range",
	"MAX_LEARING_RATE" : 1e-3,
	"STEP_SIZE_UP": 20,
	"GAMMA": 0.99,


Experiment with:
    Ray Tune:
        Scheduler 
        Search_algo

        tensorboard --logdir ~/ray_results



        cross validation: no augmentation? -> two different dataset -> solved

                          no shuffle? -> torch SubsetRandomSampler have already shuffled indexes in each epoch. 


        output0.txt: 
            hyperparameter tunning 
        output1.txt:
            cross validation


features: save the best results





modify inner_outer fold for tensorboard writer and logdir



best performance:
    trial 139: resnet18 5 fold
    trial 143: resnet18: 10 fold
        
Hyper parameter tunning: 
    learning rate -> regularization 
                     learning rate decay
                     model size

Ensemble learning
https://discuss.pytorch.org/t/custom-ensemble-approach/52024/4

    
Three tasks:
    Inspect dataset: 
        
        Two ID subject for 1 image: 
            # SEARCH_METRICS = "ID_SUBJECT" # "ID_IMAGE"
            # ID_IMAGE = "I218391"
            # ID_SUBJECT = "037_V_4001" 

            -> SOLVED
        
        Two image for 1 subject & 1 image id: 
            SEARCH_METRICS = "ID_IMAGE"
            ID_IMAGE = "I41449"
            ID_SUBJECT = "094_S_1241"'

            -> SOLVED
    
    Predict on all dataset (done) -> create data frame with probability and class and true/false (done) -> save as csv format (done)
        -> choose false images -> discard

    Train image: convert file to image (done) 
    


    Retrain model with new image data: .nii -> torch

    Inspect augmentation on MONAI library (done)

    Inspect max, min, value range, std, ...

    Inspect weight and model structures: 

    Create ensemble model

on 27/12/2021
1. Nested 5 fold cross validation
2. CAM for visualization

3. Viết chọn hình bằng máy

600 ảnh
    200 ảnh (máy): ảnh distinct 
    400 ảnh: lọc bỏ 
        400 ảnh dán nhãn sẳn
            bác sĩ làm gì khi gặp ảnh khó
                chụp lại ảnh khác hay tự bác sĩ chuẩn đoán
                    -> ảnh + sinh thiết + MMSE
            đưa 400 ảnh chưa được chuẩn đoán: hạn chế cần được phát triển (limitation)


"/mnt/data_lab513/vqtran_data/data/data_train_dec/tensor/x_tensor_NC_EMCI_AD_cv_data_filter.pt" 